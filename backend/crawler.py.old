import asyncio
import json
import logging
import os
from datetime import datetime, timedelta
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError, Error as PlaywrightError
from typing import Optional, List, Dict, Any # For type hinting
import re
import pytz
from utils.config import Config

# Assuming ConnectionManager is defined in app.py or a shared module
# If not, you might need to adjust the import or pass it differently.
# from app import ConnectionManager # Example import

MAX_RETRIES = 2 # Total attempts = 1 + MAX_RETRIES
RETRY_DELAY = 1 # Seconds between retries
DEFAULT_TIMEOUT = 4000 # 4 seconds in milliseconds for Playwright actions
NAVIGATION_TIMEOUT = 10000 

# Control characters for BiDi: RTL override / Pop directional formatting
BIDI_RTL = "\u202B"
BIDI_POP = "\u202C"

# Configure logging (ensure it's configured globally or passed appropriately)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class HolmesPlaceCrawler:
    """Handles crawling Holmes Place website for class schedules."""
    # Add type hint for websocket_manager if possible (e.g., from app import ConnectionManager)
    def __init__(self, base_url="https://www.holmesplace.co.il/", 
                 output_dir=None, headless=False, clubs_filter=None,
                 websocket_manager=None, stop_event=None, clubs_to_process=None):
        """Initializes the HolmesPlaceCrawler with configuration settings."""
        self.base_url = base_url
        self.output_dir = output_dir or Config.OUTPUT_DIR
        self.headless = headless
        self.clubs_filter = clubs_filter
        self.browser = None
        self.page = None
        self.found_classes = []
        self.processed_clubs = {}
        self.club_to_region = {}
        self.club_to_opening_hours = {}  # Store opening hours for each club
        self.club_to_classes = {}  # Store classes by club
        self.club_to_address = {}  # Store club addresses
        self.club_start_times = {}  # Track when crawl for each club started
        self.club_end_times = {}    # Track when crawl for each club ended
        self.stop_event = stop_event if stop_event else asyncio.Event()
        self.total_clubs = 0
        self.current_club_index = 0
        self.websocket_manager = websocket_manager
        self.clubs_to_process = clubs_to_process
        self.crawl_results = {}  # Initialize crawl_results dictionary
        # Ensure output directory exists
        os.makedirs(self.output_dir, exist_ok=True)
        # Store screenshot directory
        self.screenshot_dir = os.path.join(os.path.dirname(str(self.output_dir)), "screenshots")
        os.makedirs(self.screenshot_dir, exist_ok=True)
        # Use a single output JSONL file for all crawls
        self.filename = os.path.join(self.output_dir, "holmes.jsonl")
        self.collected_class_data = [] # Store all class data in memory

    async def _launch_browser(self):
        """Launch a new browser instance."""
        logger.info("Attempting to launch browser...")
        try:
            playwright = await async_playwright().start()
            logger.info(f"Launching Chromium (headless={self.headless})...")
            browser = await playwright.chromium.launch(
                headless=self.headless,
                args=['--disable-gpu', '--no-sandbox', '--disable-dev-shm-usage']
            )
            logger.info("Browser launched successfully.")
            return browser
        except Exception as e:
            logging.error(f"Failed to launch browser: {str(e)}", exc_info=True)
            raise

    async def _send_status(self, type: str, message: str, data: Optional[Dict[str, Any]] = None):
        """Helper to send status updates via WebSocket and log with proper Hebrew display."""
        # Reverse Hebrew segments for correct readability
        def adjust_message(msg):
            # If contains Hebrew characters, reverse order
            if re.search('[\u0590-\u05FF]', msg):
                return msg[::-1]
            return msg
        adj_message = adjust_message(message)
        if self.websocket_manager:
            status_data = {
                "type": type,
                "message": adj_message,
                "timestamp": datetime.now().isoformat(),
            }
            if data:
                status_data["data"] = data
            try:
                # Use broadcast method if ConnectionManager has it
                if hasattr(self.websocket_manager, 'broadcast'):
                     await self.websocket_manager.broadcast(json.dumps(status_data))
                else:
                    logging.warning("Websocket manager does not have a broadcast method.")
            except Exception as ws_err:
                logging.error(f"Failed to send WebSocket status: {ws_err}")
                # Handle disconnection case specifically
                if "disconnected" in str(ws_err).lower() or "connection" in str(ws_err).lower():
                    logging.warning("WebSocket appears to be disconnected. Will continue crawling but some status updates may be missed.")
                    # Store message in a buffer if possible
                    if not hasattr(self, '_status_buffer'):
                        self._status_buffer = []
                    # Buffer up to 100 messages to avoid memory issues
                    if len(self._status_buffer) < 100:
                        self._status_buffer.append(status_data)
                    # Don't try to reconnect here - just note the disconnect and continue crawling
        logging.info(f"Status Update ({type}): {adj_message} {data or ''}")

    async def _take_screenshot(self, stage_name: str):
        """Takes a screenshot for debugging and returns the path."""
        if self.page:
            try:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"error_{stage_name}_{timestamp}.png"
                path = os.path.join(self.screenshot_dir, filename)
                await self.page.screenshot(path=path)
                logging.info(f"Screenshot saved to {path}")
                await self._send_status('error_screenshot', f"שגיאה בשלב {stage_name}. צילום מסך נשמר.")
                return filename  # Return just the filename, not the full path
            except Exception as ss_error:
                logging.error(f"Failed to take screenshot: {ss_error}")
                return None
        else:
            logging.warning("Cannot take screenshot, page object is not available.")
            return None

    async def _check_stop_event(self, context_msg=""):
        """Checks if the stop event is set and raises an exception if it is."""
        if self.stop_event and self.stop_event.is_set():
            logging.warning(f"Stop event detected during: {context_msg}. Stopping crawl.")
            raise asyncio.CancelledError(f"Crawler stop requested during {context_msg}")

    async def _retry_operation(self, operation: callable, description: str):
        """Attempts an async operation with retries, checking stop event."""
        last_exception = None
        for attempt in range(MAX_RETRIES + 1):
            await self._check_stop_event(f"retry attempt {attempt+1} for {description}")
            try:
                result = await operation()
                if attempt > 0:
                     logging.info(f"Operation '{description}' succeeded on attempt {attempt + 1}.")
                return result
            except (PlaywrightTimeoutError, PlaywrightError, asyncio.TimeoutError, asyncio.CancelledError, Exception) as e:
                if isinstance(e, asyncio.CancelledError): raise e
                last_exception = e
                logging.warning(f"Attempt {attempt + 1}/{MAX_RETRIES + 1} failed for: {BIDI_RTL}{description}{BIDI_POP}. Error: {type(e).__name__} - {str(e)}")
                if attempt < MAX_RETRIES:
                    await self._check_stop_event(f"delay before retry {attempt+2} for {description}") 
                    await asyncio.sleep(RETRY_DELAY)
                else:
                    logging.error(f"{BIDI_RTL}Operation failed after {MAX_RETRIES + 1} attempts: {description}{BIDI_POP}. Final Error: {type(e).__name__} - {str(e)}")
                    await self._send_status('error', f"הפעולה '{description}' נכשלה סופית לאחר {MAX_RETRIES + 1} ניסיונות.")
                    
                    # Take a screenshot and store the path
                    screenshot_path = await self._take_screenshot(f"failed_{description.replace(' ', '_')}")
                    
                    # Check if we're processing a club and store the error details
                    club_name = None
                    
                    # Try to extract club name from the description
                    if "for " in description and " for " not in description:
                        parts = description.split("for ")
                        if len(parts) > 1:
                            club_name = parts[1].strip()
                    elif " for " in description:
                        parts = description.split(" for ")
                        if len(parts) > 1:
                            club_name = parts[1].strip()
                            
                    # If we identified a club, store the error details in crawl_results
                    if club_name and club_name in self.crawl_results:
                        error_message = f"Operation '{description}' failed: {type(e).__name__} - {str(e)}"
                        self.crawl_results[club_name]['error_reason'] = error_message
                        if screenshot_path:
                            self.crawl_results[club_name]['screenshot'] = screenshot_path
                            
                    raise last_exception # Re-raise the last captured exception
        raise RuntimeError(f"Operation '{description}' failed unexpectedly without raising an exception after retries.")

    async def _close_interfering_modal(self, description: str):
        """Checks for and closes a potentially interfering modal."""
        # --- GUESSING SELECTORS - ADJUST IF YOU KNOW THEM --- #
        interfering_modal_selector = "div.modal.fade.show[id]:not(#select-club)" # Generic visible modal, not our target one
        close_button_selector = f"{interfering_modal_selector} button.close, {interfering_modal_selector} [aria-label='Close']" # Common close buttons
        # --- END GUESSING --- #
        
        try:
            modal_element = await self.page.query_selector(interfering_modal_selector)
            if modal_element and await modal_element.is_visible():
                logging.warning(f"Interfering modal detected ({description}). Attempting to close.")
                await self._send_status('warning', f"זוהה חלון קופץ שמפריע ({description}). מנסה לסגור...")
                close_button = await self.page.query_selector(close_button_selector)
                if close_button:
                    await self._retry_operation(
                         lambda: close_button.click(timeout=2000), # Quick timeout for close click
                         f"Click close on interfering modal ({description})"
                    )
                    await self.page.wait_for_timeout(500) # Wait briefly for close animation
                    logging.info(f"Clicked close on interfering modal ({description}).")
                else:
                    logging.warning(f"Could not find close button ({close_button_selector}) for interfering modal.")
                    await self._take_screenshot(f"interfering_modal_no_close_btn_{description}")
            # else: No interfering modal found or it's not visible
        except Exception as e:
            logging.error(f"Error checking/closing interfering modal ({description}): {e}")
            # Don't fail the whole crawl, just log it.
        
    async def start(self):
        """Start the crawling process"""
        logger.info("HolmesPlaceCrawler.start() called.")
        try:
            await self.websocket_manager.update_status(
                is_running=True,
                progress=0,
                current_club=None,
                error=None
            )
            
            # Initialize browser and page
            logger.info("Calling _launch_browser...")
            self.browser = await self._launch_browser()
            if not self.browser:
                 logger.error("_launch_browser returned None. Cannot proceed.")
                 raise Exception("Browser launch failed critically.")
                 
            logger.info("Browser initialized. Creating new page...")
            self.page = await self.browser.new_page()
            logger.info("New page created.")
            
            # Navigate to the website
            logger.info(f"Navigating to base URL: {self.base_url}...")
            await self.page.goto(self.base_url)
            logger.info("Navigation to base URL complete.")
            
            # Process each club
            total_clubs = len(self.clubs_to_process)
            for i, club in enumerate(self.clubs_to_process):
                await self.websocket_manager.update_status(
                    progress=(i / total_clubs) * 100,
                    current_club=club["name"]
                )
                await self._process_club(club)
            
            await self.websocket_manager.update_status(
                is_running=False,
                progress=100,
                current_club=None
            )
            
        except Exception as e:
            await self.websocket_manager.update_status(
                is_running=False,
                error=str(e)
            )
            logger.error(f"Error during HolmesPlaceCrawler.start(): {str(e)}", exc_info=True)
            raise
        finally:
            logger.info("HolmesPlaceCrawler.start() finished (in finally block).")
            if self.browser:
                logger.info("Closing browser...")
                await self.browser.close()
                logger.info("Browser closed.")

    async def _process_club_schedule(self, club_name: str) -> int:
        """Processes schedules for a single club page."""
        logging.info(f"Processing schedule page for club: {club_name}")
        
        schedule_link_selector = 'a.btn-orange:has-text("למערכת השיעורים המלאה")'
        # Fallback link selector - alternative link to schedule
        fallback_link_selector = 'a.hp-button-link:has-text("לצפייה במערכת השיעורים")'
        # Selector for the main content area holding the schedule
        schedule_content_selector = "#pills-tab-studioContent"
        # Selector for an individual class item, used to confirm content loaded
        class_item_selector = "div.time.box-day" 
        
        any_classes_processed = False # Track if at least one class was saved for the club
        error_details = None # Track error details for failed clubs
        
        try:
            # --- Try to click the Full Schedule Link --- 
            logging.info(f"Looking for full schedule link: {schedule_link_selector}")
            try:
                await self._retry_operation(
                    lambda: self.page.locator(schedule_link_selector).click(),
                    f"Click 'Full Schedule' link for {club_name}"
                )
                logging.info(f"Clicked full schedule link for {club_name}.")
            except Exception as primary_link_err:
                # Try the fallback link if the primary one fails
                error_message = f"Failed to find primary schedule link: {str(primary_link_err)}"
                logging.warning(f"Failed to find primary schedule link for {club_name}: {primary_link_err}")
                error_details = error_message
                
                logging.info(f"Trying fallback schedule link: {fallback_link_selector}")
                
                try:
                    await self._retry_operation(
                        lambda: self.page.locator(fallback_link_selector).click(),
                        f"Click fallback 'View Schedule' link for {club_name}"
                    )
                    logging.info(f"Clicked fallback schedule link for {club_name}.")
                except Exception as fallback_link_err:
                    error_message = f"Failed to find both primary and fallback schedule links: {str(fallback_link_err)}"
                    logging.error(error_message)
                    error_details = error_message
                    raise Exception(error_message)
            
            logging.info(f"Waiting for schedule content...")
            
            # --- Wait for Schedule Content to Load --- 
            # Wait for the main container OR the first class item to appear
            # Using schedule_content_selector first as it might appear before items within it
            try:
                logging.info(f"Waiting for schedule content container: {schedule_content_selector}")
                await self._retry_operation(
                    lambda: self.page.wait_for_selector(schedule_content_selector, state='visible', timeout=DEFAULT_TIMEOUT * 1.5), # Slightly longer timeout
                    f"Wait for schedule content container '{schedule_content_selector}' for {club_name}"
                )
                logging.info(f"Schedule content container '{schedule_content_selector}' found. Verifying class items...")
                # Add a small delay AFTER container is visible, before checking items inside
                await self.page.wait_for_timeout(500)
                # Now ensure at least one class item exists within it
                await self._retry_operation(
                    lambda: self.page.wait_for_selector(f"{schedule_content_selector} {class_item_selector}", state='visible', timeout=DEFAULT_TIMEOUT),
                    f"Wait for first class item '{class_item_selector}' within container for {club_name}"
                )
                logging.info(f"First class item found within schedule container.")
            except Exception as wait_err:
                error_message = f"Failed to load schedule content: {str(wait_err)}"
                logging.error(f"Failed to find schedule content ({schedule_content_selector} or {class_item_selector}) after clicking link for {club_name}: {wait_err}")
                error_details = error_message
                
                # Take a screenshot of the failure
                screenshot_path = await self._take_screenshot(f"schedule_content_load_fail_{club_name}")
                
                # Store error details and screenshot in crawl results
                if club_name in self.crawl_results:
                    self.crawl_results[club_name]['error_reason'] = error_message
                    self.crawl_results[club_name]['screenshot'] = screenshot_path
                
                await self._send_status('error', f"תוכן מערכת השעות לא נטען לאחר לחיצה על הקישור בסניף {club_name}")
                # Consider this a failure for the club if content doesn't load
                raise wait_err # Re-raise to trigger finally block correctly

            # --- Process Schedule Content (Simplified - No Day Clicking) --- 
            logging.info(f"Processing all schedule content within {schedule_content_selector} for {club_name}")
            # Call the refactored processing function (which now handles multiple days)
            classes_found_count = await self._process_schedule_content(club_name, schedule_content_selector)
            
            if classes_found_count > 0:
                any_classes_processed = True
            else:
                error_details = "No classes found in schedule content"

        except Exception as e:
            # Catch errors during the club schedule processing
            error_message = f"Critical error processing schedule: {type(e).__name__} - {str(e)}"
            logging.error(f"Critical error processing schedule for club {club_name}: {type(e).__name__} - {e}", exc_info=False) 
            
            # If we don't already have more specific error details
            if not error_details:
                error_details = error_message
                
                # Take a screenshot of the generic error
                screenshot_path = await self._take_screenshot(f"schedule_processing_error_{club_name}")
                
                # Store error details and screenshot in crawl results  
                if club_name in self.crawl_results:
                    self.crawl_results[club_name]['error_reason'] = error_message
                    self.crawl_results[club_name]['screenshot'] = screenshot_path
            
            await self._send_status('error', f"שגיאה קריטית בעיבוד מערכת סניף {club_name}: {str(e)}")
            # Failure is determined by any_classes_processed in finally block
        finally:
            # Store error details for the club
            if not any_classes_processed and error_details:
                if club_name in self.crawl_results:
                    self.crawl_results[club_name]['error_reason'] = error_details
                else:
                    self.crawl_results[club_name] = {
                        "status": "failed",
                        "error_reason": error_details
                    }
            
            # Send final status for THIS club based on whether ANY classes were processed
            if any_classes_processed:
                await self._send_status('club_success', f"עיבוד סניף {club_name} הסתיים (נמצאו שיעורים)", {"club_name": club_name})
                logging.info(f"Club {club_name} marked as SUCCESS as some classes were processed.")
            else:
                # Only mark as failed if NO classes were processed 
                await self._send_status('club_failed', f"עיבוד סניף {club_name} נכשל (לא נמצאו שיעורים)", {"club_name": club_name})
                logging.warning(f"Club {club_name} marked as FAILED as no classes were processed.")
            return 0 if not any_classes_processed else classes_found_count

    async def _process_schedule_content(self, club_name: str, container_selector: str) -> int:
        """Extracts classes from the entire schedule content area. Handles multiple days within."""
        total_classes_found = 0
        logging.info(f"Extracting all classes within container '{container_selector}' for {club_name}...")

        # Selector for the columns/divs that group classes by day
        day_column_selector = "div.col-sm.text-center"
        day_header_selector = "div.day.sticky"
        class_item_selector = "div.time.box-day"

        try:
            day_columns = await self.page.query_selector_all(f"{container_selector} {day_column_selector}")
            if not day_columns:
                logging.warning(f"No day columns found using selector '{container_selector} {day_column_selector}' for {club_name}.")
                await self._send_status('warning', f"לא נמצאו עמודות ימים לעיבוד עבור {club_name}")
                return 0
                
            logging.info(f"Found {len(day_columns)} potential day columns to process for {club_name}.")

            for day_col_index, day_column in enumerate(day_columns):
                hebrew_day_name = f"Unknown Day {day_col_index + 1}" # Default
                iso_date_str = None # YYYY-MM-DD format
                day_classes_found_count = 0
                
                try:
                    # Extract Day Name and Date from sticky header
                    day_header_element = await day_column.query_selector(day_header_selector)
                    if day_header_element:
                        day_text_content = await day_header_element.text_content()
                        if day_text_content:
                            day_text_content = day_text_content.strip()
                            # Try to extract date first using regex
                            date_match = re.search(r'(\d{2}/\d{2}/\d{4})', day_text_content)
                            
                            if date_match:
                                date_str = date_match.group(1)
                                # Convert DD/MM/YYYY to ISO format YYYY-MM-DD
                                try:
                                    day, month, year = date_str.split('/')
                                    iso_date_str = f"{year}-{month}-{day}"
                                except Exception as date_parse_err:
                                    logging.warning(f"Error parsing date '{date_str}' for {club_name}: {date_parse_err}")
                                
                            # Extract Hebrew day name after removing the date if present
                            day_without_date = day_text_content
                            if date_match:
                                day_without_date = day_text_content.replace(date_match.group(0), '')
                                
                            # Now extract the Hebrew day name
                            day_name_match = re.search(r'(ראשון|שני|שלישי|רביעי|חמישי|שישי|שבת)', day_without_date)
                            if day_name_match:
                                hebrew_day_name = day_name_match.group(1)
                    
                    # If we couldn't extract a date but have a day name, try to calculate the date
                    if not iso_date_str and hebrew_day_name != f"Unknown Day {day_col_index + 1}":
                        # Map Hebrew days to numbers (0=Sunday, 6=Saturday)
                        hebrew_to_weekday = {
                            "ראשון": 0, "שני": 1, "שלישי": 2, "רביעי": 3,
                            "חמישי": 4, "שישי": 5, "שבת": 6
                        }
                        if hebrew_day_name in hebrew_to_weekday:
                            # Get today and calculate the date for the given day
                            today = datetime.now()
                            day_num = hebrew_to_weekday[hebrew_day_name]
                            days_ahead = (day_num - today.weekday()) % 7
                            future_date = today + timedelta(days=days_ahead)
                            iso_date_str = future_date.strftime("%Y-%m-%d")
                    
                    # After all attempts, if we don't have a date, create a fallback one
                    if not iso_date_str:
                        # Create a default date based on today + column index
                        iso_date_str = (datetime.now() + timedelta(days=day_col_index)).strftime("%Y-%m-%d")
                        logging.warning(f"Using fallback date {iso_date_str} for column {day_col_index+1} in {club_name}")
                    
                    logging.info(f"--- Processing Day Column {day_col_index+1}: {hebrew_day_name} (Date: {iso_date_str}) ---")
                    await self._send_status('day_processing', f"מעבד את יום {hebrew_day_name}", {"day": hebrew_day_name})
                    
                    # Now find all class items within this day column
                    class_elements = await day_column.query_selector_all(class_item_selector)
                    if not class_elements:
                        logging.warning(f"No class elements found in day column {day_col_index+1} ({hebrew_day_name}) for {club_name}")
                        continue

                    logging.info(f"Found {len(class_elements)} class elements for day '{hebrew_day_name}'.")

                    # Process each class element within this day column
                    for i, class_element in enumerate(class_elements):
                        try:
                            # --- Extract Data using new selectors --- 
                            class_name_element = await class_element.query_selector('h5.bottom-details')
                            time_duration_element = await class_element.query_selector('span.top-title-d')
                            details_elements = await class_element.query_selector_all('div.bottom-details p')

                            class_name = await self._retry_operation(lambda: class_name_element.text_content(), f"Get class name (day {hebrew_day_name}, item {i+1})") if class_name_element else None
                            time_duration_text = await self._retry_operation(lambda: time_duration_element.text_content(), f"Get time/duration (day {hebrew_day_name}, item {i+1})") if time_duration_element else None

                            instructor = None
                            location = None
                            if len(details_elements) >= 1:
                                p_text_1 = await self._retry_operation(lambda: details_elements[0].text_content(), f"Get details P1 (day {hebrew_day_name}, item {i+1})")
                                if p_text_1 and p_text_1.startswith("מדריך:"):
                                    instructor = p_text_1.replace("מדריך:", "").strip()
                                else:
                                    location = p_text_1.strip() 
                            if len(details_elements) >= 2:
                                p_text_2 = await self._retry_operation(lambda: details_elements[1].text_content(), f"Get details P2 (day {hebrew_day_name}, item {i+1})")
                                if location is None:
                                     location = p_text_2.strip()
                                elif instructor is None and p_text_2.startswith("מדריך:"):
                                    instructor = p_text_2.replace("מדריך:", "").strip()
                                elif not (p_text_1 and p_text_1.startswith("מדריך:")):
                                     location = p_text_2.strip()
                                     
                            class_time = None
                            duration = None
                            if time_duration_text:
                                parts = time_duration_text.split('|')
                                if len(parts) > 0:
                                    time_part = parts[0].replace('<i class="fas fa-chevron-left rotate"></i>', '').strip()
                                    if ':' in time_part:
                                         class_time = time_part
                                if len(parts) > 1:
                                    duration_part = parts[1].strip()
                                    duration_digits = ''.join(filter(str.isdigit, duration_part))
                                    if duration_digits:
                                        duration = f"{duration_digits} דק'"

                            # Clean extracted text
                            class_name = class_name.strip() if class_name else ""
                            instructor = instructor.strip() if instructor else ""
                            location = location.strip() if location else ""
                            class_time = class_time.strip() if class_time else ""
                            duration = duration.strip() if duration else ""

                            # Basic validation
                            if not class_time or not class_name:
                                logging.warning(f"Skipping class element {i+1} for day {hebrew_day_name} with missing time ('{class_time}') or name ('{class_name}')")
                                continue
                                
                            # --- SAVE TO JSONL with standard date --- 
                            class_data = {
                                "club": club_name,
                                "day": iso_date_str, # Use YYYY-MM-DD date
                                "day_name_hebrew": hebrew_day_name, # Store Hebrew name separately
                                "time": class_time,
                                "name": class_name,
                                "instructor": instructor,
                                "duration": duration,
                                "location": location,
                                "timestamp": datetime.now().isoformat()
                            }
                            
                            # Store in both collections
                            self.collected_class_data.append(class_data)
                            
                            # Also add to club-specific collection
                            if club_name in self.club_to_classes:
                                self.club_to_classes[club_name].append(class_data)
                            else:
                                self.club_to_classes[club_name] = [class_data]
                                
                            day_classes_found_count += 1
                            logging.debug(f"Saved class: {class_name} at {class_time} on {hebrew_day_name} ({iso_date_str}) in {club_name}")

                        except Exception as item_error:
                            logging.error(f"Error processing class item {i+1} for day {hebrew_day_name}: {type(item_error).__name__} - {item_error}", exc_info=True)
                            await self._send_status('warning', f"שגיאה בעיבוד פריט שיעור {i+1} ביום {hebrew_day_name}")
                            continue # Continue with the next class item within the day
                            
                    # Send status after processing all items for the day
                    await self._send_status(
                        'classes_found', 
                        f"נמצאו {day_classes_found_count} שיעורים ביום {hebrew_day_name}",
                        {"club": club_name, "day": hebrew_day_name, "classes_count": day_classes_found_count}
                    )
                    total_classes_found += day_classes_found_count
                    logging.info(f"Finished processing day '{hebrew_day_name}' ({iso_date_str}). Found {day_classes_found_count} classes.")

                except Exception as day_error:
                    logging.error(f"Error processing day column {day_col_index+1} for {club_name}: {type(day_error).__name__} - {day_error}", exc_info=True)
                    await self._send_status('warning', f"שגיאה בעיבוד יום {hebrew_day_name} בסניף {club_name}")
                    continue # Continue to the next day column
            
            logging.info(f"Finished processing all day columns for {club_name}. Total classes found: {total_classes_found}")
            return total_classes_found

        except Exception as e:
            error_msg = f"Error extracting classes for {club_name}: {type(e).__name__} - {e}"
            logging.error(error_msg, exc_info=True)
            await self._send_status('error', f"שגיאה בעיבוד שיעורים עבור {club_name}: {str(e)}")
            return 0

    async def _extract_club_opening_hours(self, club_name):
        """Extract opening hours information from the club page."""
        logging.info(f"Extracting opening hours for club: {club_name}")
        opening_hours = {}
        
        try:
            # Target the club details box
            club_details_selector = "div.club-details-box"
            club_details_element = await self.page.query_selector(club_details_selector)
            
            if not club_details_element:
                logging.warning(f"Club details box not found for {club_name}")
                return opening_hours
                
            # Find the club hours section which has the heading "שעות פעילות המועדון:"
            hours_selector = "div.club-details-info:has(h3:text('שעות פעילות המועדון:'))"
            hours_element = await club_details_element.query_selector(hours_selector)
            
            if not hours_element:
                logging.warning(f"Opening hours section not found for {club_name}")
                return opening_hours
                
            # Extract the hours from the <big> tag
            hours_text_element = await hours_element.query_selector("big")
            if not hours_text_element:
                logging.warning(f"Opening hours text element not found for {club_name}")
                return opening_hours
                
            hours_text = await hours_text_element.text_content()
            if not hours_text:
                logging.warning(f"No opening hours text found for {club_name}")
                return opening_hours
                
            # Process the hours text which is typically formatted with line breaks
            hours_lines = hours_text.strip().split('\n')
            
            for line in hours_lines:
                line = line.strip()
                if not line:
                    continue
                    
                # Try to parse each line into day and hours
                # Format is typically "day: hours" or "day hours"
                if ':' in line:
                    day_part, hours_part = line.split(':', 1)
                    opening_hours[day_part.strip()] = hours_part.strip()
                elif '-' in line:  # Look for time ranges like "06:00-23:00"
                    # Try to find the last occurrence of a letter before the time
                    parts = re.split(r'(\d+:\d+)', line, 1)
                    if len(parts) >= 2:
                        day_part = parts[0].strip()
                        hours_part = ''.join(parts[1:]).strip()
                        opening_hours[day_part] = hours_part
                else:
                    # Just store the whole line if we can't parse it
                    opening_hours[f"info_{len(opening_hours)}"] = line
            
            logging.info(f"Extracted opening hours for {club_name}: {opening_hours}")
            # Save to the crawler's dictionary
            self.club_to_opening_hours[club_name] = opening_hours
            return opening_hours
            
        except Exception as e:
            logging.error(f"Error extracting opening hours for {club_name}: {e}")
            return opening_hours

    async def _extract_club_address(self, club_name):
        """Extract address information from the club page."""
        logging.info(f"Extracting address for club: {club_name}")
        address = ""
        
        try:
            # Look for the address in the contact info section
            address_selector = "div.club-details-info.contact-info a[href*='waze.com'] i.fas.fa-map-marker-alt"
            address_element = await self.page.query_selector(address_selector)
            
            if address_element:
                # Get the parent <a> element
                parent_element = await address_element.evaluate_handle("el => el.parentElement")
                if parent_element:
                    address_text = await parent_element.text_content()
                    # Clean up the address text (remove the icon text)
                    if address_text:
                        address = address_text.replace("ברק בן אבינועם", "").strip()
                        
            if address:
                logging.info(f"Found address for {club_name}: {address}")
                self.club_to_address[club_name] = address
            else:
                logging.warning(f"Could not find address for {club_name}")
                
        except Exception as e:
            logging.error(f"Error extracting address for {club_name}: {e}")
            
        return address

    async def _process_club(self, club_info):
        """Processes a single club, extracting schedules and saving data."""
        club_name = club_info["name"]
        club_url = club_info["url"]
        
        # Record start time for this club
        self.club_start_times[club_name] = datetime.now().isoformat()
        
        # Initialize crawl results for this club
        self.crawl_results[club_name] = {
            "status": "processing",
            "url": club_url,  # Save the URL
            "error_reason": None
        }
        
        try:
            await self._retry_operation(
                lambda: self.page.goto(club_url, wait_until="networkidle"),
                f"Navigate to club page: {club_url}"
            )
            
            # Determine area based on club name
            if "ירושלים" in club_name or "מבשרת" in club_name or "מודיעין" in club_name:
                area = "ירושלים והסביבה"
            elif "תל אביב" in club_name or "רמת גן" in club_name or "בני ברק" in club_name or "גבעתיים" in club_name or "דיזנגוף" in club_name or "עזריאלי" in club_name or "פתח תקווה" in club_name or "ראש העין" in club_name or "גבעת שמואל" in club_name or "קריית אונו" in club_name:
                area = "מרכז"
            elif "חיפה" in club_name or "קריות" in club_name or "קריון" in club_name or "גרנד קניון" in club_name or "חדרה" in club_name or "קיסריה" in club_name or "נהריה" in club_name:
                area = "צפון"
            elif "באר שבע" in club_name or "אשדוד" in club_name or "אילת" in club_name:
                area = "דרום"
            elif "נתניה" in club_name or "הרצליה" in club_name or "רעננה" in club_name or "כפר סבא" in club_name or "שבעת הכוכבים" in club_name or "הוד השרון" in club_name:
                area = "שרון"
            elif "רחובות" in club_name or "ראשון לציון" in club_name or "נס ציונה" in club_name or "לוד" in club_name or "יבנה" in club_name:
                area = "שפלה"
            else:
                area = "מרכז"  # Default to מרכז instead of אחר
            
            # Store area for this club
            self.club_to_region[club_name] = area
            
            # Extract opening hours and address before processing the schedule
            opening_hours = await self._extract_club_opening_hours(club_name)
            address = await self._extract_club_address(club_name)
            
            # Save opening hours to crawl results
            self.crawl_results[club_name]["opening_hours"] = opening_hours
            self.crawl_results[club_name]["address"] = address
            
            # Initialize an empty list for this club's classes
            self.club_to_classes[club_name] = []
            
            # Process the class schedule
            classes_count = await self._process_club_schedule(club_name)
            
            # Record end time for this club
            self.club_end_times[club_name] = datetime.now().isoformat()
            
            # Update status to success
            self.crawl_results[club_name]["status"] = "success"
            self.crawl_results[club_name]["classes_count"] = classes_count
            
        except Exception as e:
            logging.error(f"Error processing club {club_name}: {str(e)}")
            self.crawl_results[club_name]["status"] = "failed"
            self.crawl_results[club_name]["error_reason"] = str(e)
            raise

    def _calculate_duration(self, start_time_str, end_time_str):
        """Calculate duration in seconds between two ISO format timestamps"""
        if not start_time_str or not end_time_str:
            return None
            
        try:
            start_time = datetime.fromisoformat(start_time_str)
            end_time = datetime.fromisoformat(end_time_str)
            duration = (end_time - start_time).total_seconds()
            return duration
        except Exception as e:
            logging.warning(f"Error calculating duration: {e}")
            return None
    
    def get_results(self):
        """Returns a dictionary of all clubs with their classes."""
        results = {}
        
        # Create consolidated results with one entry per club
        for club_name, club_data in self.crawl_results.items():
            # Create club entry with all available data
            club_entry = {
                "club_name": club_name,
                "address": self.club_to_address.get(club_name, ""),
                "opening_hours": self.club_to_opening_hours.get(club_name, ""),
                "area": self.club_to_region.get(club_name, ""),
                "start_time": self.club_start_times.get(club_name, ""),
                "end_time": self.club_end_times.get(club_name, ""),
                "duration": self._calculate_duration(
                    self.club_start_times.get(club_name), 
                    self.club_end_times.get(club_name)
                ),
                "status": club_data['status']
            }
            
            # For successful clubs, include class data
            if club_data['status'] == 'success':
                club_entry["classes"] = self.club_to_classes.get(club_name, [])
            # For failed clubs, include error info and screenshot link if available
            else:
                club_entry["classes"] = []  # Empty array for failed clubs
                club_entry["error_reason"] = club_data.get('error_reason', "Unknown error")
                
                # Check for screenshot link in crawl_results
                if 'screenshot' in club_data and club_data['screenshot']:
                    # Include full URL for screenshot
                    base_url = "http://localhost:8080/screenshots/"
                    club_entry["screenshot_url"] = base_url + club_data['screenshot']
                # If no screenshot in crawl_results, look in screenshots directory
                else:
                    screenshots_dir = os.path.join(os.path.dirname(self.output_dir), "screenshots")
                    if os.path.exists(screenshots_dir):
                        # Look for screenshots with the club name in them
                        club_screenshots = [
                            f 
                            for f in os.listdir(screenshots_dir) 
                            if club_name in f and f.endswith(".png")
                        ]
                        if club_screenshots:
                            # Sort by timestamp (newest first) and take the most recent one
                            club_screenshots.sort(reverse=True)
                            # Include full URL for screenshot
                            base_url = "http://localhost:8080/screenshots/"
                            club_entry["screenshot_url"] = base_url + club_screenshots[0]
            
            results[club_name] = club_entry
            
        return results

# Example usage (if run directly)
async def main():
    crawler = HolmesPlaceCrawler(headless=True)  # Enable headless mode
    await crawler.start()

if __name__ == "__main__":
    # To run this crawler directly for testing:
    # Ensure you have a running asyncio event loop
    # You might need to install playwright browsers: python -m playwright install
    asyncio.run(main()) 